{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE \n",
    "### 2. Fill the blanks with T (True) or F (False) for the statements given above:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Maximum likelihood estimation provides not only point estimation, but a\n",
    "distribution information of the parameters estimated. (T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Maximum likelihood and Bayesian approachs for parameter estimation perform well with low-dimensional dataset with many training examples while\n",
    "their performance is bad on high-dimensional dataset with few training examples. (T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "### 1. Consider that you are given the dataset in the table above consisting of boolean variables x, y and z and a single boolean output variable C. Suppose that the Naive Bayes classifier is going to be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Specify the value of P(C = 1|x = 1, y = 1, z = 0). Show your solution step\n",
    "by step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(C=1|x=1,y=1,z=0) = P(x=1|C=1)*P(y=1|C=1)*P(z=0|C=1)*P(C=1) = 1/2 * 1/4 * 1/2 * 1/2 = 1/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Specify the value of P(C = 0|x = 1, y = 1). Show your solution step by step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(C=0|x=1,y=1) = P(x=1|C=0)*P(y=1|C=0)*P(C=0) = 1/2 * 1/2 * 1/2 = 1/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that the Joint Naive Bayes classifier is used for the options below:\n",
    "\n",
    "• Specify the value of P(C = 1|x = 1, y = 1, z = 0). Show your solution step\n",
    "by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x=1, y=1, z=0  C=0  P(C=1|x=1,y=1,z=0) = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Specify the value of P(C = 0|x = 1, y = 1). Show your solution step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x=1,y=1,C=0   x=1,y=1,C=1  P(C=0|x=1,y=1)= 1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II: Detection of Fake News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   INTRODUCTIONS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a classifier?\n",
    "\n",
    "A classifier is a machine learning model that is used to discriminate different objects based on certain features.\n",
    "\n",
    "Principle of Naive Bayes Classifier:\n",
    "\n",
    "A Naive Bayes classifier is a probabilistic machine learning model that’s used for classification task. The crux of the classifier is based on the Bayes theorem. Naive Bayes algorithms are mostly used in sentiment analysis, spam filtering, recommendation systems etc. They are fast and easy to implement but their biggest disadvantage is that the requirement of predictors to be independent. In most of the real life cases, the predictors are dependent, this hinders the performance of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "With a multinomial event model, samples (feature vectors) represent the frequencies with which certain events have been generated by a multinomial ${\\displaystyle (p_{1},\\dots ,p_{n})}$ where ${\\displaystyle p_{i}}$ is the probability that event i occurs (or $K$ such multinomials in the multiclass case). A feature vector ${\\displaystyle \\mathbf {x} =(x_{1},\\dots ,x_{n})}$ is then a histogram, with ${\\displaystyle x_{i}}$ counting the number of times event $i$ was observed in a particular instance. This is the event model typically used for document classification, with events representing the occurrence of a word in a single document (e.g Bag Of Words assumption). The likelihood of observing a histogram $x$ is given by\n",
    "\n",
    "${\\displaystyle p(\\mathbf {x} \\mid C_{k})={\\frac {(\\sum _{i}x_{i})!}{\\prod _{i}x_{i}!}}\\prod _{i}{p_{ki}}^{x_{i}}}$\n",
    "\n",
    "The multinomial naive Bayes classifier becomes a linear classifier when expressed in log-space:[2]\n",
    "\n",
    "${\\displaystyle {\\begin{aligned}\\log p(C_{k}\\mid \\mathbf {x} )&\\varpropto \\log \\left(p(C_{k})\\prod _{i=1}^{n}{p_{ki}}^{x_{i}}\\right)\\\\&=\\log p(C_{k})+\\sum _{i=1}^{n}x_{i}\\cdot \\log p_{ki}\\\\&=b+\\mathbf {w} _{k}^{\\top }\\mathbf {x} \\end{aligned}}}$\n",
    "\n",
    "where ${\\displaystyle b=\\log p(C_{k})}$ and ${\\displaystyle w_{ki}=\\log p_{ki}}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace Smoothing in Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $c$ refer to a class (such as Positive or Negative), and let $w$ refer to a token or word.\n",
    "\n",
    "The maximum likelihood estimator for $P(w|c)$ is\n",
    "\n",
    "$$   \\frac{count(w,c)}{count(c)}= \\frac{counts\\,w\\,in\\,class\\,c}{counts\\,of\\,words\\,in\\,class\\,c}$$\n",
    "\n",
    "\n",
    "This estimation of $P(w|c)$ could be problematic since it would give us probability $0$ for documents with unknown words. A common way of solving this problem is to use Laplace smoothing.\n",
    "\n",
    "Let $V$ be the set of words in the training set, add a new element $UNK$ (for unknown) to the set of words.\n",
    "\n",
    "Define\n",
    "\n",
    "$$   P(w|c) = \\frac{count(w,c)+1}{count(c)+|V|+1}$$\n",
    "\n",
    "where $V$ refers to the vocabulary (the words in the training set).\n",
    "\n",
    "In particular, any unknown word will have probability\n",
    "\n",
    "$$   P(w|c) = \\frac{1}{count(c)+|V|+1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from NaiveBayes import naive_bayes\n",
    "from test import run_test, All_Results\n",
    "from utility import understanding_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First of all we have to call desired libraries.\n",
    " \n",
    " Numpy is a python library which provides matrix operations.\n",
    " \n",
    " Pandas stands for “Python Data Analysis Library” a must library for data analysis.\n",
    " \n",
    " The CountVectorizer function is a real game changer when it comes to implementing the naive bayes algorithm. Here is more information about CountVectorization https://towardsdatascience.com/natural-language-processing-count-vectorization-with-scikit-learn-e7804269bb5e\n",
    "\n",
    "And then we got our stop words and my own libraries\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " [1_YEJf9BQQh0ma1ECs6x_7yQ.png](attachment:1_YEJf9BQQh0ma1ECs6x_7yQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fake_news_train.csv',sep=\",\")\n",
    "\n",
    "df = df.drop(columns = ['id','author','text'])\n",
    "df = df[pd.notnull(df['title'])]\n",
    "#test_size ve random state degistirilebilir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we read our fake_news_train.csv file and drop 'id','author' and 'title' because we will not need them in our current experiment. We drop the titles which are null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df['title']\n",
    "Y_train = df['label']\n",
    "dumbx, X_test, dumby, Y_test = (\n",
    "    train_test_split(df['title'],df['label'],test_size=0.05,random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not have to split our data to train and test. Therefore I get all data to my X_train and Y_train.\n",
    "But unfortunately by getting some errors I decided to use train_test_split() function to get my X_test and Y_test datas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        FLYNN: Hillary Clinton, Big Woman on Campus - ...\n",
      "1        15 Civilians Killed In Single US Airstrike Hav...\n",
      "2        Jackie Mason: Hollywood Would Love Trump if He...\n",
      "3        Life: Life Of Luxury: Elton John’s 6 Favorite ...\n",
      "4        Benoît Hamon Wins French Socialist Party’s Pre...\n",
      "                               ...                        \n",
      "16635    Rapper T.I.: Trump a ’Poster Child For White S...\n",
      "16636    N.F.L. Playoffs: Schedule, Matchups and Odds -...\n",
      "16637    Macy’s Is Said to Receive Takeover Approach by...\n",
      "16638    NATO, Russia To Hold Parallel Exercises In Bal...\n",
      "16639                            What Keeps the F-35 Alive\n",
      "Name: title, Length: 16194, dtype: object\n",
      "0        0\n",
      "1        1\n",
      "2        0\n",
      "3        1\n",
      "4        0\n",
      "        ..\n",
      "16635    0\n",
      "16636    0\n",
      "16637    0\n",
      "16638    1\n",
      "16639    1\n",
      "Name: label, Length: 16194, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15475    In the Shadow of a Fairy Tale - The New York T...\n",
      "15645    Get Ready For Civil Unrest: Survey Finds That ...\n",
      "8111     In Russian Doping Scandal, Time for a Punishme...\n",
      "10794         Friday Fox Follies – Megyn & Murdoch’s Money\n",
      "13326    Johnny Nicholson, Whose Midtown Cafe Drew the ...\n",
      "                               ...                        \n",
      "742      3 Reasons Why You Should Apply For A Job In Th...\n",
      "11559                           The Factless Fact-Checkers\n",
      "5257     This Is Why America Will Get the President it ...\n",
      "9784     Clinton Staffer Caught Sacrificing Baby to Dem...\n",
      "14343                Is Western Civilization Worth Saving?\n",
      "Name: title, Length: 810, dtype: object\n",
      "15475    0\n",
      "15645    1\n",
      "8111     0\n",
      "10794    1\n",
      "13326    0\n",
      "        ..\n",
      "742      1\n",
      "11559    1\n",
      "5257     1\n",
      "9784     1\n",
      "14343    1\n",
      "Name: label, Length: 810, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_test)\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_array = X_test.to_numpy()\n",
    "Y_test_array = Y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Understanding The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Word/WordPairs\n",
      "0 s\n",
      "1 trump\n",
      "2 hillary\n",
      "  Word/WordPairs\n",
      "0 new\n",
      "1 york\n",
      "2 times\n"
     ]
    }
   ],
   "source": [
    "P_word_real,P_word_fake,P_real,P_fake,uniq_len,cv = naive_bayes(X_train,Y_train,(1,1),True)\n",
    "understanding_data(P_word_real,P_word_fake,cv,3,\"real\")\n",
    "understanding_data(P_word_real,P_word_fake,cv,3,\"fake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the most appeared words in fake news is 3 words \"new\" \"york\" \"times\" :)\n",
    "It was really amazing to have that result from all that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Implementing Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will se the result of using the unigram or bigram model for the naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_word_real,P_word_fake,P_real,P_fake,uniq_len,cv = naive_bayes(X_train,Y_train,(1,1),True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call our naive_bayes function by passing X_train_cv,Y_train_array and getting the trained parameters for our model. After that we can do our predictions really fast by using just those paramethers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply Count Vecotrizer to our X_train_cv we do not have to apply cv to others. The X_train_cv is so big that it gives Memory Error when we try to convert it to array. Therefore it stores it as a sparse matrix format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_World_real is a vector which contains all the conditional probabilities for each word like \n",
      "[P(\"ali\"|real),P(\"ahmet\"|real),P(\"veli\"|real),P(\"hasan\"|real)]\n",
      "\n",
      "[4.08405733e-04 7.42555877e-05 7.42555877e-05 ... 7.42555877e-05\n",
      " 7.42555877e-05 7.42555877e-05]\n",
      "\n",
      "P_World_fake is a vector which contains all the conditional probabilities for each word like \n",
      "[P(\"ali\"|fake),P(\"ahmet\"|fake),P(\"veli\"|fake),P(\"hasan\"|fake)]\n",
      "\n",
      "[3.65363537e-05 3.65363537e-05 3.65363537e-05 ... 3.65363537e-05\n",
      " 3.65363537e-05 3.65363537e-05]\n",
      "\n",
      "P_real is the total real cases in the all dataset:\n",
      "26934\n",
      "\n",
      "P_real is the total real cases in the all dataset:\n",
      "27370\n"
     ]
    }
   ],
   "source": [
    "print(\"P_World_real is a vector which contains all the conditional probabilities for each word like \\n[P(\\\"ali\\\"|real),P(\\\"ahmet\\\"|real),P(\\\"veli\\\"|real),P(\\\"hasan\\\"|real)]\\n\")\n",
    "print(P_word_real)\n",
    "print(\"\\nP_World_fake is a vector which contains all the conditional probabilities for each word like \\n[P(\\\"ali\\\"|fake),P(\\\"ahmet\\\"|fake),P(\\\"veli\\\"|fake),P(\\\"hasan\\\"|fake)]\\n\")\n",
    "print(P_word_fake)\n",
    "print(\"\\nP_real is the total real cases in the all dataset:\")\n",
    "print(P_real)\n",
    "print(\"\\nP_real is the total real cases in the all dataset:\")\n",
    "print(P_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application results are below ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram, Stopwords: No\n",
      "Correct classified news:557 out of 810\n",
      "Accuracy: 68.76543209876543\n",
      "####################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_word_real,P_word_fake,P_real,P_fake,uniq_len,cv = naive_bayes(X_train,Y_train,(1,1),True)\n",
    "\n",
    "run_test(\"Unigram, Stopwords: No\",X_test_array,Y_test_array,P_word_real, P_word_fake, P_real, P_fake, uniq_len ,cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the bigram approach we treat two words as a \"word\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application results are below ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram, Stopwords: No\n",
      "Correct classified news:434 out of 810\n",
      "Accuracy: 53.58024691358024\n",
      "####################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_word_real,P_word_fake,P_real,P_fake,uniq_len,cv = naive_bayes(X_train,Y_train,(2,2),True)\n",
    "\n",
    "run_test(\"Bigram, Stopwords: No\",X_test_array,Y_test_array,P_word_real, P_word_fake, P_real, P_fake, uniq_len ,cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:\n",
    "### (a) Analyzing effect of the words on prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I did the only analysis for the binary situations because the absence words in the false news that strengthened the possibility real news, likewise other situations include cross possibility.\n",
    "\n",
    "I tried it in both cases unigram and Bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_word_real,P_word_fake,P_real,P_fake,uniq_len,cv = naive_bayes(X_train,Y_train,(1,1),True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 words whose presence most strongly predicts that the news is real.\n",
    "And whose absence most strongly predicts that the news is fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Word/WordPairs\n",
      "0 s\n",
      "1 trump\n",
      "2 hillary\n",
      "3 clinton\n",
      "4 election\n",
      "5 new\n",
      "6 video\n",
      "7 news\n",
      "8 russia\n",
      "9 war\n"
     ]
    }
   ],
   "source": [
    "understanding_data(P_word_real,P_word_fake,cv,10,\"real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 words whose absence most strongly predicts that the news is real.\n",
    "And whose presence most strongly predicts that the news is fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Word/WordPairs\n",
      "0 new\n",
      "1 york\n",
      "2 times\n",
      "3 s\n",
      "4 breitbart\n",
      "5 trump\n",
      "6 donald\n",
      "7 u\n",
      "8 t\n",
      "9 obama\n"
     ]
    }
   ],
   "source": [
    "understanding_data(P_word_real,P_word_fake,cv,10,\"fake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_word_real,P_word_fake,P_real,P_fake,uniq_len,cv = naive_bayes(X_train,Y_train,(2,2),True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 words whose presence most strongly predicts that the news is real.\n",
    "And whose absence most strongly predicts that the news is fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Word/WordPairs\n",
      "0 hillary clinton\n",
      "1 donald trump\n",
      "2 trump s\n",
      "3 u s\n",
      "4 america s\n",
      "5 hillary s\n",
      "6 clinton s\n",
      "7 world war\n",
      "8 onion america\n",
      "9 finest news\n"
     ]
    }
   ],
   "source": [
    "understanding_data(P_word_real,P_word_fake,cv,10,\"real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 words whose absence most strongly predicts that the news is real.\n",
    "And whose presence most strongly predicts that the news is fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Word/WordPairs\n",
      "0 new york\n",
      "1 york times\n",
      "2 donald trump\n",
      "3 trump s\n",
      "4 u s\n",
      "5 briefing new\n",
      "6 hillary clinton\n",
      "7 evening briefing\n",
      "8 white house\n",
      "9 dies new\n"
     ]
    }
   ],
   "source": [
    "understanding_data(P_word_real,P_word_fake,cv,10,\"fake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I already have done my analysis by not using stopwords. Lets make a analyse if we not exlude the stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application results are below ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram, Stopwords: yes\n",
      "Correct classified news:588 out of 810\n",
      "Accuracy: 72.5925925925926\n",
      "####################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_word_real,P_word_fake,P_real,P_fake,uniq_len,cv = naive_bayes(X_train,Y_train,(1,1),False)\n",
    "\n",
    "run_test(\"Unigram, Stopwords: yes\",X_test_array,Y_test_array,P_word_real, P_word_fake, P_real, P_fake, uniq_len ,cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the bigram approach we treat two words as a \"word\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application results are below ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram, Stopwords: yes\n",
      "Correct classified news:440 out of 810\n",
      "Accuracy: 54.32098765432099\n",
      "####################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_word_real,P_word_fake,P_real,P_fake,uniq_len,cv = naive_bayes(X_train,Y_train,(2,2),False)\n",
    "\n",
    "run_test(\"Bigram, Stopwords: yes\",X_test_array,Y_test_array,P_word_real, P_word_fake, P_real, P_fake, uniq_len ,cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Including Stopwords\n",
    "### (a) Analyzing effect of the words on prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I did the only analysis for the binary situations because the absence words in the false news that strengthened the possibility real news, likewise other situations include cross possibility.\n",
    "\n",
    "I tried it in both cases unigram and Bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_word_real,P_word_fake,P_real,P_fake,uniq_len,cv = naive_bayes(X_train,Y_train,(1,1),False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 words whose presence most strongly predicts that the news is real.\n",
    "And whose absence most strongly predicts that the news is fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Word/WordPairs\n",
      "0 the\n",
      "1 to\n",
      "2 of\n",
      "3 s\n",
      "4 in\n",
      "5 trump\n",
      "6 a\n",
      "7 and\n",
      "8 on\n",
      "9 for\n"
     ]
    }
   ],
   "source": [
    "understanding_data(P_word_real,P_word_fake,cv,10,\"real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 words whose absence most strongly predicts that the news is real.\n",
    "And whose presence most strongly predicts that the news is fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Word/WordPairs\n",
      "0 the\n",
      "1 new\n",
      "2 york\n",
      "3 times\n",
      "4 to\n",
      "5 s\n",
      "6 breitbart\n",
      "7 trump\n",
      "8 in\n",
      "9 of\n"
     ]
    }
   ],
   "source": [
    "understanding_data(P_word_real,P_word_fake,cv,10,\"fake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_word_real,P_word_fake,P_real,P_fake,uniq_len,cv = naive_bayes(X_train,Y_train,(2,2),False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 words whose presence most strongly predicts that the news is real.\n",
    "And whose absence most strongly predicts that the news is fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Word/WordPairs\n",
      "0 hillary clinton\n",
      "1 comment on\n",
      "2 of the\n",
      "3 donald trump\n",
      "4 in the\n",
      "5 trump s\n",
      "6 u s\n",
      "7 america s\n",
      "8 on the\n",
      "9 and the\n"
     ]
    }
   ],
   "source": [
    "understanding_data(P_word_real,P_word_fake,cv,10,\"real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 words whose absence most strongly predicts that the news is real.\n",
    "And whose presence most strongly predicts that the news is fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Word/WordPairs\n",
      "0 new york\n",
      "1 the new\n",
      "2 york times\n",
      "3 donald trump\n",
      "4 trump s\n",
      "5 u s\n",
      "6 briefing the\n",
      "7 hillary clinton\n",
      "8 in the\n",
      "9 at the\n"
     ]
    }
   ],
   "source": [
    "understanding_data(P_word_real,P_word_fake,cv,10,\"fake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Results & Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have all our results here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram, Stopwords: No\n",
      "Accuracy: 68.76543209876543\n",
      "Correct classified news: 557 out of 810\n",
      "####################################\n",
      "Bigram, Stopwords: No\n",
      "Accuracy: 53.58024691358024\n",
      "Correct classified news: 434 out of 810\n",
      "####################################\n",
      "Unigram, Stopwords: yes\n",
      "Accuracy: 72.5925925925926\n",
      "Correct classified news: 588 out of 810\n",
      "####################################\n",
      "Bigram, Stopwords: yes\n",
      "Accuracy: 54.32098765432099\n",
      "Correct classified news: 440 out of 810\n",
      "####################################\n"
     ]
    }
   ],
   "source": [
    "All_Results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Stopword-yes Stopword-no\n",
    "\n",
    "Unigram  <strong>72.5925</strong>     68.7654\n",
    "\n",
    "Bigram     54.3209     53.5802\n",
    "\n",
    "| Stopword? | Yes     | No      |\n",
    "|-----------|---------|---------|\n",
    "| Unigram   | 72.5925 | 68.7654 |\n",
    "| Bigram    | 54.3209 | 53.5802 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result Unigram gave better result than Bigram and not excluding stopwords gave better result than excluding them. It can be maybe because of our training dataset is relatively small. Because of just using titles, unigram gave better results than bigram.\n",
    "The best accuracy in our model is <strong>72.5925</strong>\n",
    "\n",
    "We can see that Naive Bayes Classifier is relatively easy to implement and it can be really useful for Natural Language Processing. Even the Training part can be a very long but once we train it we can do everything by using the result probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                          $$\\\\Ismet\\\\OKATAR\\\\21727542$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
